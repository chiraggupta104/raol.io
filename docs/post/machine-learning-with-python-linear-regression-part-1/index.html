<!DOCTYPE html>
<!--[if lt IE 7]><html class="no-js lt-ie9 lt-ie8 lt-ie7"> <![endif]-->
<!--[if IE 7]><html class="no-js lt-ie9 lt-ie8"> <![endif]-->
<!--[if IE 8]><html class="no-js lt-ie9"> <![endif]-->
<!--[if gt IE 8]><!--><html class="no-js" prefix="og: http://ogp.me/ns#" xmlns:og="http://ogp.me/ns#"><!--<![endif]-->

    <head>
                <meta charset="utf-8">
        <meta http-equiv="X-UA-Compatible" content="IE=edge">
        <meta name="description" content="">
        <meta name="viewport" content="width=device-width, initial-scale=1.0, minimum-scale=1.0, maximum-scale=1.0" />
        <meta name="mobile-web-app-capable" content="yes">
        <meta name="apple-mobile-web-app-capable" content="yes">
        <meta name="apple-mobile-web-app-status-bar-style" content="black">
        <meta name="author" content="Oleg Rakitskiy">
  
	
        <meta property="og:site_name" content="Raol.io">
        <meta property="og:title" content="Raol.io">
        <meta property="og:url" content="http://example.org/post/machine-learning-with-python-linear-regression-part-1/">
        <meta property="og:description" content="Thoughts on software engineering.">
    
        <meta property="og:type" content="article" />
        <meta property="og:article:author" content="Oleg Rakitskiy" />
        <meta property="og:article:published_time" content="2014-04-15T00:00:00Z" />
    
        <meta name="generator" content="Hugo 0.21-DEV" />
        <title>Machine learning with Python. Linear regression. Part 1 &middot; Raol.io</title>
        <link rel="canonical" href="http://example.org/" />
        <link rel="alternate" type="application/rss+xml" title="RSS" href="">
        <link rel="stylesheet" type="text/css" href="http://example.org/css/main.css"/>
        <link href="/css/prism.css" rel="stylesheet" />
        <script src="/js/prism.js"></script>
        <link href="//fonts.googleapis.com/css?family=Source+Sans+Pro:300|Montserrat:700" rel="stylesheet" type="text/css">
        <link href="//netdna.bootstrapcdn.com/font-awesome/4.0.3/css/font-awesome.css" rel="stylesheet">
        <script src="//code.jquery.com/jquery-1.10.2.min.js"></script>
        <script type="text/x-mathjax-config">
            MathJax.Hub.Config({
                tex2jax: {inlineMath: [['$','$']]}
            });
        </script>
        <script type="text/javascript" async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML">
    </script>
    </head>

<body>
<!--[if lt IE 7]><p class="browsehappy">You are using an <strong>outdated</strong> browser. Please <a href="http://browsehappy.com/">upgrade your browser</a> or <a href="http://www.google.com/chrome/â€Ž">install Google Chrome</a> to experience this site.</p><![endif]-->

    <header id="site-header">
        <div class="container">
            <a href="http://example.org/" alt="Raol.io"><h1 class="blog-title heading">Raol.io</h1></a>
            <p class="blog-description">Thoughts on software engineering.</p>
            
                <li class="navigation_item">
                    <a href="http://twitter.com/raoldev" title="@raoldev on Twitter"> <i class='fa fa-twitter'></i> <span class="label">@raoldev</span></a>
                </li>
            
            
            
                <li class="navigation_item">
                    <a href="https://github.com/raol" title="raol on github"> <i class='fa fa-github'></i> <span class="label">raol</span> </a>
                </li>
             
        </div>
    </header>
<main class="content" role="main">
	<div class="container">
		<article class="post">
	<header class="post-header">
        <h3 class="p-post-title">Machine learning with Python. Linear regression. Part 1</h3>
    </header>

    <section class="post-content">
        <p>I&rsquo;m taking cool <a href="https://www.coursera.org/course/ml">Machine Learning</a> class at Coursera, so to get better understanding of the material,
decided to write a series of blog posts about it. As it clear from the post title, I&rsquo;m going to explain what linear regression is and how it works.
I don&rsquo;t like to read long-long posts because somewhere in the middle I can&rsquo;t get rid of feeling that the beginning of the post became vague and most of
facts were forgotten. So it&rsquo;s going to be short and painless.</p>

<p></p>

<h3 id="linear-regression">Linear regression</h3>

<p>Linear regression is the way to model relationship between input variables $ X $ and output values $ Y $ using linear function. Our model fits in form
$$Y = \theta_0 + \theta_1*X_1 + &hellip; +\theta_n*X_n$$
where $ \theta_n&rsquo;s $ are coefficients for linear function above.
So when function is found it&rsquo;s possible to predict $ Y $ value for new $ X $ that does not fall in range of known inputs.</p>

<p>Suppose you have several stores across the country and going to open couple new ones. You need to estimate how profitable will it be if it&rsquo;s opened in one city or another. The data describing profit of existing stores are presented below as a bunch of points at the scatter plot</p>

<p><img src="/assets/images/linear_regression_1/plain_data.png" alt="plain data" /></p>

<p>Now linear parameter must be chosen to draw a straight line lying as close to each point as possible. In other words our goal is to minimize cost function $ J(\theta) $:
$$ J(\theta) = \frac{1}{2m}\sum_{i=0}^{n} (h_\theta(x^{(i)}) - y^{(i)})^2 $$
where $ h_\theta(x) $ is our linear function and paramters are $ \theta $ values.</p>

<p>To find the mininum (global or local, it depends on function) of the function we will use <a href="http://en.wikipedia.org/wiki/Gradient_descent">gradient descent</a> method. It is based on on fact that differentiable function decreases in the gradient direction. So basic formula to adjust $ \theta $ parameters using batch gradient descend is:
$$ \theta_j = \theta_j - \alpha \frac{\partial}{\partial \theta_j} J(\theta_j) $$
If you have taken calculus course it is easy to calculate partial derivative by hand, but don&rsquo;t worry if you didn&rsquo;t get it. Below is calculated derivatives and formula for the $ \theta_i $ update.
$$ \frac{\partial}{\partial \theta_j} J(\theta_j) = \frac{1}{m}\sum_{i=0}^{n} (h_\theta(x^{(i)}) - y^{(i)}) \space \text{if j = 0} $$</p>

<p>it is for the first coefficient $ \theta_0 $</p>

<p>$$ \frac{\partial}{\partial \theta_j} J(\theta_j) = \frac{1}{m}\sum_{i=0}^{n} (h_\theta(x^{(i)}) - y^{(i)})x_{j}^{(i)} \space \text{if j &gt; 0} $$</p>

<p><strong>Important point</strong>: all $ \theta $ values should be updated simultaneously, i.e. we calculate all new thethas using the old ones first, store them in temporary variables and then at the end of iteration update all theta values.</p>

<p>With each gradient descent step our $ \theta $ values will be closer to the optimal values and cost function $ J(\theta) $ will be closer to its minimum.</p>

<p>Full code of the naive implementation is below.</p>

<pre><code class="language-python">import numpy as np
import matplotlib.pyplot as plt


def costFunction(theta, x, y):
    cost = 0
    for i in zip(x, y):
        cost = cost + ((h(theta, i[0]) - i[1]) ** 2);
    return cost / (2 * len(x))

def h(theta, x):
    return theta[0] + theta[1] * x

def gradientDescent(x, y):
    # initialize alpha value
    alpha = 0.01
    iternum = 1500
    # set up initial theta values
    theta = [0, 0]
    for i in range(0, iternum):
        theta_0 = 0
        theta_1 = 0
        for i in zip(x, y):
            theta_0 += (h(theta, i[0]) - i[1])
            theta_1 += (h(theta, i[0]) - i[1]) * i[0]

        theta[0] = theta[0] - alpha*theta_0 / len(x)
        theta[1] = theta[1] - alpha*theta_1 / len(x)

    return theta;

f = open('ex1data1.txt')
a = zip(*[[float(l.split(',')[0]), float(l.split(',')[1])] for l in f.readlines()])

x = a[0]
y = a[1];
minX = min(*x)
maxX = max(*x)
minY = min(*y)
maxY = max(*y)

plt.xlabel(&quot;Population of City in 10,000s&quot;)
plt.ylabel(&quot;Profit in $10K&quot;)
plt.axis([min(*x) - 1, max(*x) + 1, min(*y) - 1, max(*y) + 1])

plt.plot(x, y, 'ro', label='training data')

tt = gradientDescent(x, y)
print(tt)
xn = np.arange(minX, maxX, 1)
plt.plot(xn, tt[0] + tt[1]*xn, label='Linear function')


plt.legend(loc=2)
plt.show()
</code></pre>

<p>Final rounded theta values calculated by the code are $ \theta_0 = -3.6302 $ and $ \theta_1 = 1.1664 $</p>

<p>And our linear function with minimal distance to the training data set is shown at the plot below.</p>

<p><img src="/assets/images/linear_regression_1/function_data.png" alt="function data" /></p>

<p>Now using our theta values we can estimate profit for a city with 50K citizens. It is approximately <code>547.000 $</code></p>

<p>In my next post I&rsquo;m going to explain how to choose $ \alpha $ values, and how to implement it better (certainly it can be done better).
It&rsquo;s on the way.</p>
    </section>

    <hr>

    <footer class="post-footer">
        <section class="f-1">
            
            <section class="author">
                <p>Words by Oleg Rakitskiy</p>
            </section>
            
            
            <p class="f-post-time"><time datetime="2014-04-15T00:00:00Z">April 15, 2014</time></p>
        </section>
                        
        <section class="f-2">
            <section class="share">
                <span>Share:
                <a class="icon-twitter" href="http://twitter.com/share?text=Machine%20learning%20with%20Python.%20Linear%20regression.%20Part%201&url=http%3a%2f%2fexample.org%2fpost%2fmachine-learning-with-python-linear-regression-part-1%2f"
                    onclick="window.open(this.href, 'twitter-share', 'width=550,height=235');return false;">
                    <i class="fa fa-twitter"></i>
                </a>
                <a class="icon-facebook" href="https://www.facebook.com/sharer/sharer.php?u=http%3a%2f%2fexample.org%2fpost%2fmachine-learning-with-python-linear-regression-part-1%2f"
                    onclick="window.open(this.href, 'facebook-share','width=580,height=296');return false;">
                    <i class="fa fa-facebook"></i>
                </a>
                <a class="icon-google-plus" href="https://plus.google.com/share?url=http%3a%2f%2fexample.org%2fpost%2fmachine-learning-with-python-linear-regression-part-1%2f"
                   onclick="window.open(this.href, 'google-plus-share', 'width=490,height=530');return false;">
                    <i class="fa fa-google-plus"></i>
                </a>
                </span>
            </section>

            
            	<span class="f-post-tags"><i class="fa fa-tag"></i>
            	python, machine learning
            	</span>
            
        </section>
                        
    </footer>
</article>
	</div>
</main>
    <footer id="site-footer">
        <div class="container">
          
          <a href="http://example.org/index.xml" title="Get the RSS feed"><span class="tooltip"><i class="fa fa-rss"></i></span></a>
          <section>&copy; <a href="http://example.org/">Oleg Rakitskiy</a> 2017 | All rights reserved</section>
        </div>
    </footer>

    <script type="text/javascript" src="http://example.org/js/fittext.js"></script>
    <script type="text/javascript">
      $(".heading").fitText();
    </script>


</body>
</html>